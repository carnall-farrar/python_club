{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT7mL9C96PDJ"
      },
      "source": [
        "# Aggregating Data with Group by\n",
        "\n",
        "In this session we will explore data aggregation using pandas.  Aggregation means splitting a dataset up into parts and applying mathematical or other functions to those groups of data.\n",
        "\n",
        "By the end you will be able to create a monthly time-series showing numbers of referrals, separated by priority level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjP8ds8Q6PDJ"
      },
      "source": [
        "## Theory \n",
        "\n",
        "You may have come across the concept of data aggregation in a SQL course, and the principle is exactly the same here.\n",
        "\n",
        "The 'group by' operation splits the data into groups based on entries in one or more columns, so that operations or calculations can be performed on subgroups of the data.\n",
        "\n",
        "For example, say we wanted to calculate the total number of referrals made in each CCG for some time period. Our dataset looks like this:\n",
        "\n",
        "| ccg_code | specialty | priority | referrals |\n",
        "| --- | --- | --- | --- | \n",
        "| 00L | Cardiology | Routine | 200 |\n",
        "| 00L | Urology | Routine | 600 |\n",
        "| 99C | Cardiology | Urgent | 100 |\n",
        "| 00L | 2WW | 2 Week Wait | 400 |\n",
        "| 99C | Neurology | Routine | 900 |\n",
        "\n",
        "The data contains more information than we need, because we are not interested in the specialty or priority. In which case we want to sum referrals **for each CCG**, summing over the values in other columns. In other words, we want to *group by* CCG and calculate the sum:\n",
        "\n",
        "| ccg_code | referrals |\n",
        "| --- | --- | \n",
        "| 00L | 1,200 |\n",
        "| 99C | 1,000 |\n",
        "\n",
        "## Sections\n",
        "\n",
        "* [Group by a single column](#item1)\n",
        "* [Group by multiple columns](#item2)\n",
        "* [Group by units of time](#item3)\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AzjTUM6e6PDK"
      },
      "outputs": [],
      "source": [
        "# Importing Python packages we are likely to need\n",
        "import pandas as pd  # useful for reading and manipulating data tables\n",
        "import matplotlib.pyplot as plt  # useful for plotting data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wQKmEqOF6PDK"
      },
      "outputs": [],
      "source": [
        "# We assume input and output data are in your machine's Downloads folder\n",
        "input_data_path = \"https://github.com/carnall-farrar/python_club/blob/master/data/referrals_oct19_dec20.csv\"\n",
        "output_data_path = \"https://github.com/carnall-farrar/python_club/blob/master/data/referrals_by_priority.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMxIqV4m6PDK"
      },
      "source": [
        "### Load the data\n",
        "\n",
        "* Use `pd.read_csv` to read in the data. \n",
        "* Then use `pd.to_datetime` to convert the `week_start` column to datetime format, e.g.\n",
        "\n",
        "```python\n",
        "df['my_date_column'] = pd.to_datetime(df['my_date_column'])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GsiK8c2A_eYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "AW8dsuJY6PDK"
      },
      "source": [
        "---\n",
        "<a name=\"item1\">\n",
        "<h2>Group by a single column</h2>\n",
        "</a>\n",
        "\n",
        "First, we'll take a subset of the data which will be easier to work with. Select rows where `Specialty` column has value `'2WW'` (two week wait cancer referrals).\n",
        "* Use `==` to make a selection mask\n",
        "* Use `df.loc` to select rows where the mask values are `True`\n",
        "\n",
        "```python\n",
        "smaller_df = df.loc[df['column'] == 'value', :]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FK3GGP_Z_c5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "QgydfriW6PDK"
      },
      "source": [
        "### Sum 2WW referrals, aggregated by CCG\n",
        "* Use `df.groupby('ccg_code')`\n",
        "* Select the `referrals` column\n",
        "* Use the `sum` method\n",
        "\n",
        "```python\n",
        "gb = df.groupby('aggregation_column')\n",
        "summed_values = gb['value_column'].sum()\n",
        "```\n",
        "\n",
        "The result of the aggregation operation (i.e. the sum) is a pandas `Series` object. The series index comprises the CCG codes and the values are the summed referrals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eONGCdgp6PDL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "0BUOD5Oi6PDL"
      },
      "source": [
        "### Visualise aggregation result\n",
        "Numbers are great, but we like pictures better. Let's visualise the referrals in each CCG with a bar chart. Use your aggregation result from above and `matplotlib`. \n",
        "\n",
        "```python\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(df.index, df.values)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnggQvR06PDL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Hj1CwWA-6PDL"
      },
      "source": [
        "### Take the average weekly referrals, for each CCG\n",
        "Sum is not the only aggregation function you can use. Let's try the same `groupby` result you made previously, but this time use `mean` instead of `sum`.\n",
        "\n",
        "```python\n",
        "avg_values = gb['value_column'].mean()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZIvBWIe6PDL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "WVmFnxE46PDL"
      },
      "source": [
        "### Other useful aggregate functions\n",
        "* `count`\n",
        "* `median`\n",
        "* `min` and `max`\n",
        "* (And more)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQNwYjsG6PDL"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "A client has asked: for each CCG, what is the highest number of 2WW referrals they can expect to see in a single week?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rniwCiHH6PDL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDdAehUB6PDL"
      },
      "source": [
        "---\n",
        "<a name=\"item2\"><h2>\n",
        "Group by multiple columns\n",
        "</h2></a>\n",
        "\n",
        "It's possible to group by multiple columns at once. This means you apply the aggregation function to each subset of values in the columns you group by. For example, if you group by `CCG` and `priority` then use the `sum` function, you will get the total number of referrals per CCG *and* per priority level.\n",
        "\n",
        "```python\n",
        "gb_multi_cols = df.groupby(['column_1', 'column_2'])\n",
        "summed_values = gb_multi_cols['value_colum'].sum()\n",
        "```\n",
        "The result is still a pandas `Series` object, but this time the index has two 'levels', one for each aggregation column.\n",
        "\n",
        "Let's group by both `specialty` and `priority`, then take the sum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4sQwX2M6PDL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "6saloKq56PDL"
      },
      "source": [
        "---\n",
        "<a name=\"item3\"><h2>Group by units of time</h1></a>\n",
        "\n",
        "You can group by `datetime` values, just like anything else. But sometimes the units of time might be too granular to be useful, e.g. your `datetime` values may be expressed at the level of minutes or seconds, but if your data span a year then you probably don't want to show each individual data point.\n",
        "\n",
        "Pandas has a special date `Grouper` object, which you provide to `df.groupby`. Its `key` argument lets you specify the column you want to group by and the `freq` argument specifies the desired frequency, where\n",
        "* `D` means daily\n",
        "* `W` means weekly\n",
        "* `M` means monthly\n",
        "\n",
        "The full list of frequency options can be found <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases\">here</a>.\n",
        "\n",
        "```python\n",
        "gb_daily = df.groupby(pd.Grouper(key='column_name', freq='D'))\n",
        "agg_df = gb_daily['value_column'].sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NedImBo6PDL"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Find the total number of referrals by month and then plot the resulting time series, then plot with `df.plot()`. What happened in March 2020?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apyOYwdF6PDL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "KcnvOg536PDL"
      },
      "source": [
        "### Unstacking\n",
        "\n",
        "We saw above that grouping by multiple columns at once gives us a pandas `Series` object with two index 'levels'. But it is easier to handle the data if one of these levels can be switched around and transformed into multiple *columns* instead. In pandas this is called an `unstack` operation, by analogy with taking a stack of books (like horizontal rows) and rotating them to place on a shelf (like vertical columns).\n",
        "\n",
        "```python\n",
        "gb_multi_cols = df.groupby(['column_1', 'column_2'])\n",
        "summed_values_stacked = gb_multi_cols['value_colum'].sum()\n",
        "summed_values_unstacked_c1 = summed_values_stacked.unstack('column_1')\n",
        "```\n",
        "#### Example\n",
        "Let's say our dataframe looked like this and then we used `groupby` with `sum`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgBgNNeT6PDL"
      },
      "outputs": [],
      "source": [
        "# define example data\n",
        "example_data = {\n",
        "    \"ccg_code\": [\"00L\", \"00L\", \"99C\", \"00L\", \"99C\"],\n",
        "    \"priority\": [\"Routine\", \"Routine\", \"Urgent\", \"2 Week Wait\", \"Routine\"],\n",
        "    \"referrals\": [\"2\", \"6\", \"1\", \"4\", \"9\"]\n",
        "}\n",
        "example = pd.DataFrame(example_data)\n",
        "example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeRT0DgO6PDL"
      },
      "outputs": [],
      "source": [
        "# aggregate example data\n",
        "gb_example_stacked = example.groupby(['ccg_code', 'priority']).sum()\n",
        "gb_example_stacked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6CsexYl6PDL"
      },
      "source": [
        "We can make the `priority` level into columns, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_WQX1J56PDL"
      },
      "outputs": [],
      "source": [
        "gb_example_stacked.unstack('priority')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u27Ar_RJ6PDL"
      },
      "source": [
        "Notice there's missing data (`NaN`) for some elements, since those combinations weren't present in the original data.\n",
        "\n",
        "#### Practice\n",
        "Now let's group by both month and priority and take the total number of referrals for each, then unstack the priority level, so that months are rows and priorities are columns. This will allow us to plot the time series for each priority simultaneously.\n",
        "\n",
        "E.g. result of the following will have rows for each date and columns for each value of `other_column`:\n",
        "\n",
        "```python\n",
        "gb_date_and_col = df.groupby([pd.Grouper(key='date_column', freq='M'), 'other_column'])\n",
        "agg_df = gb_monthly_priority['value'].sum()\n",
        "unstacked = agg_df.unstack('other_column')\n",
        "```\n",
        "\n",
        "Then we can plot these all together with `unstacked.plot()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10XDjldV6PDM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s_pWpyQ6PDM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Asz15Oi6PDM"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "The client has come back and said they meant for each **specialty**, not each priority! Also they want the time points to be the **start of each quarter**, rather than monthly.\n",
        "\n",
        "Hint: you can find the reference list of time point aggregations here: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbmYorPP6PDM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}